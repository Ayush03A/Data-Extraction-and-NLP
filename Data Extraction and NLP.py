# -*- coding: utf-8 -*-
"""Blackcoffer's Assignment Solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PFolDMuYbIphIp-QxVX3oZ-huZG-6Za8
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import os
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import string
import re

output_text_path = '/content/drive/MyDrive/Blackcoffer /Articles Text/'
input_csv_path = '/content/drive/MyDrive/Blackcoffer /Assignment test/Input.csv'
positive_txt_path = '/content/drive/MyDrive/Blackcoffer /Assignment test/positive-words.txt'
negative_txt_path = '/content/drive/MyDrive/Blackcoffer /Assignment test/negative-words.txt'
output_xl_path = "/content/drive/MyDrive/Blackcoffer /Output/Textual Analysis Output.xlsx"

# Function to scrape article title and text from a given URL
def scrape_article(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extracting article title
        title = soup.find('title').get_text()

        # Extracting article text from multiple classes
        article_text = ''
        content_div = (
            soup.find('div', class_='tdb_single_content') or
            soup.find('div', class_='td-post-content tagdiv-type') or
            soup.find('div', class_='td-main-content')
        )
        if content_div:
            paragraphs = content_div.find_all(['p', 'ol', 'li'])
            article_text = '\n'.join(p.get_text() for p in paragraphs)

        return title, article_text
    except Exception as e:
        print("Error occurred while scraping:", e)
        return None, None

# Function to save article title and text to a txt file
def save_to_txt(url_id, title, article_text):
    filename = output_text_path +f"{url_id}.txt"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(title + '\n\n')
        f.write(article_text)

# Function to perform textual analysis
def textual_analysis(text):
    # Tokenize text
    tokens = word_tokenize(text)
    sentences = sent_tokenize(text)

    # Remove punctuation and convert to lowercase
    tokens = [word.lower() for word in tokens if word.isalnum()]

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if not word in stop_words]

    # Calculate word frequency
    word_freq = nltk.FreqDist(tokens)

    # Calculate total number of words
    word_count = len(tokens)

    # Calculate number of complex words
    complex_words = [word for word in tokens if syllable_count_word(word) > 2]
    complex_word_count = len(complex_words)

    # Calculate positive score
    positive_score = sum(1 for word in tokens if word in positive_dict)

    # Calculate negative score
    negative_score = sum(1 for word in tokens if word in negative_dict)

    # Calculate polarity score
    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)

    # Calculate subjectivity score
    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)

    # Calculate average sentence length
    avg_sentence_length = word_count / len(sentences)

    # Calculate percentage of complex words
    percentage_complex_words = (complex_word_count / word_count) * 100

    # Calculate fog index
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    # Calculate average number of words per sentence
    avg_words_per_sentence = word_count / len(sentences)

    # Calculate personal pronouns count
    personal_pronouns = ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']
    personal_pronoun_count = sum(word.lower() in personal_pronouns for word in tokens)

    # Calculate average word length
    avg_word_length = sum(len(word) for word in tokens) / word_count

    # Calculate syllable per word
    syllable_per_word = sum(syllable_count_word(word) for word in tokens) / word_count

    return (positive_score, negative_score, polarity_score, subjectivity_score,
            avg_sentence_length, percentage_complex_words, fog_index,
            avg_words_per_sentence, complex_word_count, word_count, syllable_per_word,
            personal_pronoun_count, avg_word_length)

# Function to count syllables in a word
def syllable_count_word(word):
    count = 0
    vowels = 'aeiouy'
    word = word.lower()
    if word[0] in vowels:
        count += 1
    for index in range(1, len(word)):
        if word[index] in vowels and word[index - 1] not in vowels:
            count += 1
    if word.endswith('e'):
        count -= 1
    if count == 0:
        count += 1
    return count

df = pd.read_csv(input_csv_path)
df_input = (df.dropna())

# Iterate through each row in the dataframe
for index, row in df_input.iterrows():
    url = row['URL']
    url_id = row['URL_ID']

    # Scrape article title and text
    title, article_text = scrape_article(url)

    # Save to txt file
    if title and article_text:
        save_to_txt(url_id, title, article_text)
        print(f"Saved '{title}' to {url_id}.txt")
    else:
        print(f"Failed to save {url_id}.txt")

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')




# Load positive and negative dictionaries
positive_dict = set(open(positive_txt_path).read().split())
negative_dict = set(open(negative_txt_path,encoding = "ISO-8859-1").read().split())


# Initialize output data
output_data = pd.DataFrame(columns=df_input.columns.tolist() + [
    'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',
    'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',
    'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',
    'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'
])


# Get list of files in the folder
folder_files = os.listdir(output_text_path )

# Iterate through each row in the input dataframe
for index, row in df_input.iterrows():
    url = row['URL']
    url_id = row['URL_ID']

    # Check if URL ID is present in the folder
    if f"{url_id}.txt" in folder_files:
       # Read the text from the file
        with open(output_text_path +f"{url_id}.txt", "r", encoding="utf-8") as file:
            text = file.read()
        # Perform textual analysis
        if text:
            analysis_result = textual_analysis(text)

    # Prepare output data
        output_row = {key: value for key, value in zip(df_input.columns.tolist(), row)}
        output_row.update({output_data.columns.tolist()[i + len(df_input.columns)]: value
                           for i, value in enumerate(analysis_result)})

        # Append output row to output data
        # output_data = output_data.append(output_row, ignore_index=True)
        output_data = pd.concat([output_data, pd.DataFrame([output_row])], ignore_index=True)
        # # Add 'NA' to the corresponding cell in the Excel sheet
        # output_data.loc[index, 'Extracted_Text'] = 'NA'
        # continue
    else :
        output_row = {key: value for key, value in zip(df_input.columns.tolist(), row)}
        na_tuple = ('NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA')
        output_row.update({output_data.columns.tolist()[i + len(df_input.columns)]: value
                           for i, value in enumerate(na_tuple)})
        output_data = pd.concat([output_data, pd.DataFrame([output_row])], ignore_index=True)


# Save output to Excel file
output_data.to_excel(output_xl_path, index=False)

